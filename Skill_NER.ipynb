{
 "cells": [
  {
   "cell_type": "code",
   "id": "87fae193b47562c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:05:04.325022Z",
     "start_time": "2025-04-20T16:05:00.292543Z"
    }
   },
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizerFast, AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tracy/miniforge3/envs/pytorch_env/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init()"
   ],
   "id": "27955f3bebeef064"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenize Data",
   "id": "43ef21ceab839653"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:09:17.883577Z",
     "start_time": "2025-04-20T16:09:17.573782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "\n",
    "label_list = ['O', 'B-SKILL', 'I-SKILL']\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "def tokenize_and_label(row):\n",
    "    text = row[\"Qualification\"]\n",
    "    skills = row[\"Skills_Dict\"]\n",
    "\n",
    "    # tokenize the text into a dict output with input_ids and offset_mapping as keys\n",
    "    encoding = tokenizer(text,\n",
    "                         return_offsets_mapping=True,\n",
    "                         return_attention_mask=True,\n",
    "                         truncation=True,\n",
    "                         padding=\"max_length\",\n",
    "                         max_length=128)\n",
    "\n",
    "\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "    labels = [0] * len(offsets)\n",
    "\n",
    "    skill_spans = []\n",
    "    for skill in skills:\n",
    "        for match in re.finditer(r'\\b{}\\b'.format(re.escape(skill)), text):\n",
    "            skill_spans.append((match.start(), match.end()))\n",
    "\n",
    "    for span_start, span_end in skill_spans:\n",
    "        inside = False\n",
    "        for i, (token_start, token_end) in enumerate(offsets):\n",
    "            if token_start == token_end:\n",
    "                labels[i] = -100  # special token like [CLS], [SEP], [PAD]\n",
    "                continue\n",
    "\n",
    "            if token_end <= span_start:\n",
    "                continue\n",
    "            if token_start >= span_end:\n",
    "                break\n",
    "\n",
    "            if token_start < span_end and token_end > span_start:\n",
    "                if labels[i] == 0:\n",
    "                    labels[i] = 1 if not inside else 2\n",
    "                    inside = True\n",
    "\n",
    "    for i, (start, end) in enumerate(offsets):\n",
    "        if start == end:\n",
    "            labels[i] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encoding[\"input_ids\"],\n",
    "        \"attention_mask\": encoding[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "df_filtered = pd.read_csv(\"/Users/tracy/Desktop/留学/UMich/SI 630/Final Project/Data/labeled_train.csv\", index_col=0)\n",
    "df_filtered[\"Skills_Dict\"] = df_filtered[\"Skills\"].apply(ast.literal_eval)\n",
    "\n",
    "tokenized_data = df_filtered.apply(tokenize_and_label, axis=1)\n",
    "dataset = Dataset.from_list(tokenized_data.tolist())\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n"
   ],
   "id": "6645a5d81c83d3bf",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "1b912fd9a39a54b5",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9d63dc8c2d3a62c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:09:24.535916Z",
     "start_time": "2025-04-20T16:09:20.519971Z"
    }
   },
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"Jean-Baptiste/roberta-large-ner-english\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "model.to(device)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at Jean-Baptiste/roberta-large-ner-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([5, 1024]) in the checkpoint and torch.Size([3, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train and Evaluate model",
   "id": "58007d6be4139d38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:09:30.084406Z",
     "start_time": "2025-04-20T16:09:30.071999Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 10,
   "source": [
    "def extract_entities(labels):\n",
    "    entities = []\n",
    "    start = None\n",
    "    entity_type = None\n",
    "\n",
    "    for i, tag in enumerate(labels):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if start is not None:\n",
    "                entities.append((start, i - 1, entity_type))\n",
    "            start = i\n",
    "            entity_type = tag[2:]\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            if entity_type is None:\n",
    "                start = i\n",
    "                entity_type = tag[2:]\n",
    "        else:\n",
    "            if start is not None:\n",
    "                entities.append((start, i - 1, entity_type))\n",
    "                start = None\n",
    "                entity_type = None\n",
    "\n",
    "    if start is not None:\n",
    "        entities.append((start, len(labels) - 1, entity_type))\n",
    "\n",
    "    return entities\n",
    "\n",
    "def compute_custom_ner_metrics(predictions, labels, id2label):\n",
    "    total_pred = 0\n",
    "    total_true = 0\n",
    "    correct = 0\n",
    "    token_correct = 0\n",
    "    token_total = 0\n",
    "\n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        pred_labels = [id2label[p] for p in pred_seq]\n",
    "        true_labels = [id2label[l] for l in label_seq]\n",
    "\n",
    "        pred_entities = set(extract_entities(pred_labels))\n",
    "        true_entities = set(extract_entities(true_labels))\n",
    "\n",
    "        total_pred += len(pred_entities)\n",
    "        total_true += len(true_entities)\n",
    "        correct += len(pred_entities & true_entities)\n",
    "\n",
    "        for pl, tl in zip(pred_labels, true_labels):\n",
    "            if tl != \"O\":\n",
    "                token_total += 1\n",
    "                if pl == tl:\n",
    "                    token_correct += 1\n",
    "\n",
    "    precision = correct / total_pred if total_pred else 0\n",
    "    recall = correct / total_true if total_true else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n",
    "    token_acc = token_correct / token_total if token_total else 0\n",
    "\n",
    "    return {\n",
    "        \"precision\": round(precision, 4),\n",
    "        \"recall\": round(recall, 4),\n",
    "        \"f1\": round(f1, 4),\n",
    "        \"token_accuracy\": round(token_acc, 4)\n",
    "    }\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true = []\n",
    "    pred = []\n",
    "\n",
    "    for p_seq, l_seq in zip(predictions, labels):\n",
    "        filtered_preds = []\n",
    "        filtered_labels = []\n",
    "        for p, l in zip(p_seq, l_seq):\n",
    "            if l != -100:\n",
    "                filtered_preds.append(p)\n",
    "                filtered_labels.append(l)\n",
    "        pred.append(filtered_preds)\n",
    "        true.append(filtered_labels)\n",
    "\n",
    "    return compute_custom_ner_metrics(pred, true, id2label)\n"
   ],
   "id": "a17b87b40607930a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:21:05.863626Z",
     "start_time": "2025-04-20T17:21:05.821744Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 20,
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.02,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    use_cpu=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ],
   "id": "630cd5d88c26f89a"
  },
  {
   "cell_type": "code",
   "id": "31850f77b47597b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:30:04.536986Z",
     "start_time": "2025-04-20T17:21:09.064070Z"
    }
   },
   "source": [
    "trainer.train()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 08:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.087241</td>\n",
       "      <td>0.888900</td>\n",
       "      <td>0.615400</td>\n",
       "      <td>0.727300</td>\n",
       "      <td>0.593000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.094641</td>\n",
       "      <td>0.732100</td>\n",
       "      <td>0.788500</td>\n",
       "      <td>0.759300</td>\n",
       "      <td>0.872100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.068571</td>\n",
       "      <td>0.809500</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>0.723400</td>\n",
       "      <td>0.674400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.063129</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.769200</td>\n",
       "      <td>0.784300</td>\n",
       "      <td>0.814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.063887</td>\n",
       "      <td>0.784300</td>\n",
       "      <td>0.769200</td>\n",
       "      <td>0.776700</td>\n",
       "      <td>0.825600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.00454567264765501, metrics={'train_runtime': 534.8619, 'train_samples_per_second': 1.44, 'train_steps_per_second': 0.093, 'total_flos': 178776702128640.0, 'train_loss': 0.00454567264765501, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "95824adfa0ca20c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:35:50.243408Z",
     "start_time": "2025-04-20T17:35:47.124053Z"
    }
   },
   "source": [
    "trainer.evaluate(eval_dataset)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.06388738751411438,\n",
       " 'eval_precision': 0.7843,\n",
       " 'eval_recall': 0.7692,\n",
       " 'eval_f1': 0.7767,\n",
       " 'eval_token_accuracy': 0.8256,\n",
       " 'eval_runtime': 3.1075,\n",
       " 'eval_samples_per_second': 5.792,\n",
       " 'eval_steps_per_second': 0.644,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "2aea0f425bed6cc5",
   "metadata": {},
   "source": "# Predict Skill keywords"
  },
  {
   "cell_type": "code",
   "id": "edc23d23493d3859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:38:23.026601Z",
     "start_time": "2025-04-20T17:38:23.018287Z"
    }
   },
   "source": [
    "def pred_on_text(text):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_mask = encoding[\"attention_mask\"]\n",
    "    offsets = encoding[\"offset_mapping\"][0].tolist()\n",
    "\n",
    "    trainer.model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)[0].tolist()\n",
    "\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    results = []\n",
    "    for token, pred, (start, end) in zip(tokens, predictions, offsets):\n",
    "        if start == end or pred == -100:\n",
    "            continue\n",
    "        label = id2label[pred]\n",
    "        results.append((text[start:end], label))\n",
    "\n",
    "    return results\n",
    "\n",
    "def pred_label(text):\n",
    "    pred_result = pred_on_text(text)\n",
    "\n",
    "    list_skills = []\n",
    "    for p in pred_result:\n",
    "        if p[1] in [\"B-SKILL\", \"I-SKILL\"]:\n",
    "            list_skills.append(p[0])\n",
    "\n",
    "    return list_skills"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "13f11867b89ebe06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:38:32.770266Z",
     "start_time": "2025-04-20T17:38:25.084575Z"
    }
   },
   "source": [
    "df_pred = pd.read_excel(\"/Users/tracy/Desktop/留学/UMich/SI 630/Final Project/Data/summarized_test.xlsx\", index_col=0)\n",
    "df_pred = df_pred.dropna()\n",
    "df_pred.reset_index(inplace=True, drop=True)\n",
    "df_pred['Skills_Pred'] = None\n",
    "\n",
    "for idx, row in df_pred.iterrows():\n",
    "    text = row[\"Qualification\"]\n",
    "    pred_res = pred_label(text)\n",
    "    df_pred.at[idx, 'Skills_Pred'] = pred_res\n",
    "\n",
    "print(\"Finish Prediction!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Prediction!\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.push_to_hub()",
   "id": "8d5c14d5b789f780"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
