{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T02:37:38.677826Z",
     "start_time": "2025-04-21T02:37:38.669812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer"
   ],
   "id": "a5df781d402d97e2",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Build GPT from Scratch",
   "id": "2c8817fb09b3cc4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Vivid Lessons:\n",
    "1. understand what data is required for training\n",
    "2. understand the dimension of input (size of token embeddings and position embeddings should be equivalent to vocab size and block size)\n",
    "3. learn to use tokenizer flexibly, when to padd when to truncate when to add EOS\n",
    "4. Add wandb and tqdm to know how many hours needed for training and how well the model is"
   ],
   "id": "a91799079f1fa242"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Processing\n",
   "id": "4821d50f60de7aee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T02:37:38.690339Z",
     "start_time": "2025-04-21T02:37:38.684455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CoverLetterDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=200):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        prompt = (\n",
    "            f\"Job Title: {row['Job Title']}\\n\"\n",
    "            f\"Preferred Qualifications: {row['Preferred Qualifications']}\\n\"\n",
    "            f\"Hiring Company: {row['Hiring Company']}\\n\"\n",
    "            f\"Applicant Name: {row['Applicant Name']}\\n\"\n",
    "            f\"Past Working Experience: {row['Past Working Experience']}\\n\"\n",
    "            f\"Current Working Experience: {row['Current Working Experience']}\\n\"\n",
    "            f\"Skillsets: {row['Skillsets']}\\n\"\n",
    "            f\"Qualifications: {row['Qualifications']}\\n\"\n",
    "            f\"Cover Letter:\"\n",
    "        )\n",
    "        target = row['Cover Letter'] + self.tokenizer.eos_token\n",
    "        full_text = prompt + \"\\n\" + target\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            full_text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encodings.input_ids.squeeze(0)\n",
    "\n",
    "        return input_ids, input_ids.clone()\n"
   ],
   "id": "e7982c87e0731c88",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GPT2",
   "id": "37f386ac139dd780"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T02:37:38.754017Z",
     "start_time": "2025-04-21T02:37:38.739419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.out = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = q @ k.transpose(-2, -1) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        output = attn @ v\n",
    "        concat = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.attn = MaskedSelfAttention(n_embd, n_head)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.ln1(x)\n",
    "        q = k = v = x\n",
    "        attn_output, _ = self.attn(q, k, v, mask=mask)\n",
    "        x = x + attn_output\n",
    "        mlp_output = self.mlp(self.ln2(x))\n",
    "        x = x + mlp_output\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_layers, n_heads, n_embds):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_embds = n_embds\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embds)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embds)\n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(n_embds, n_heads) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embds)\n",
    "        self.lm_head = nn.Linear(n_embds, vocab_size, bias=False)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T = x.size()\n",
    "\n",
    "        tokens = self.token_embedding_table(x)\n",
    "        positions_id = torch.arange(0, x.size(1), device=x.device)\n",
    "        positions = self.position_embedding_table(positions_id)\n",
    "        x = tokens + positions\n",
    "\n",
    "        if mask is None:\n",
    "            # mask = generate_causal_mask(T, device=x.device)\n",
    "            mask = generate_causal_mask(\n",
    "            seq_len=T,\n",
    "            batch_size=B,\n",
    "            num_heads=self.blocks[0].attn.num_heads,\n",
    "            device=x.device\n",
    "        )\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask=mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_len, mask=None):\n",
    "        for _ in range(max_len):\n",
    "            # only keep last context\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "\n",
    "            # get the prediction\n",
    "            logits = self(idx_cond)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  #(batch_size, seq_len, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # randomly sample from the multinominal distribution\n",
    "            idx_next = torch.multinomial(probs, 1)\n",
    "\n",
    "            # add the sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, seq_len + 1)\n",
    "\n",
    "        return idx  # shape (B, max_len + 1)\n",
    "\n",
    "    def resize_token_embeddings(self, new_vocab_size):\n",
    "        \"\"\"\n",
    "        Resize token embeddings and output projection to match new tokenizer size.\n",
    "        Similar to Hugging Face's resize_token_embeddings.\n",
    "        \"\"\"\n",
    "        old_vocab_size = self.token_embedding_table.num_embeddings\n",
    "        if new_vocab_size == old_vocab_size:\n",
    "            return  # no need to resize\n",
    "\n",
    "        # Resize token embedding table\n",
    "        new_embedding = nn.Embedding(new_vocab_size, self.n_embds)\n",
    "        new_embedding.weight.data[:old_vocab_size] = self.token_embedding_table.weight.data\n",
    "        self.token_embedding_table = new_embedding\n",
    "\n",
    "        # Resize LM head\n",
    "        new_lm_head = nn.Linear(self.n_embds, new_vocab_size, bias=False)\n",
    "        new_lm_head.weight.data[:old_vocab_size] = self.lm_head.weight.data\n",
    "        self.lm_head = new_lm_head\n",
    "\n",
    "        self.vocab_size = new_vocab_size\n"
   ],
   "id": "1007069574ef5c63",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T02:37:38.774396Z",
     "start_time": "2025-04-21T02:37:38.771652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def generate_causal_mask(seq_len, batch_size, num_heads, device):\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, device=device)).bool()\n",
    "    return mask.unsqueeze(0).unsqueeze(0).expand(batch_size, num_heads, seq_len, seq_len)\n",
    "\n"
   ],
   "id": "ed6d67d51d12bc99",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Loop",
   "id": "f0256b7ec93a0cfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T02:37:38.785556Z",
     "start_time": "2025-04-21T02:37:38.777240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTTrainer():\n",
    "    def __init__(self, vocab_size, block_size, n_layers, n_heads, n_embds, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = GPT(vocab_size, block_size, n_layers, n_heads, n_embds)\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer)) # resize the tokenizer when manually adding pad token\n",
    "\n",
    "    def train(self, loader, num_epochs, lr, save_filename='GPT2TW.pt', device='cuda'):\n",
    "        \"\"\"\n",
    "        Main training loop\n",
    "        :param loader: The training data loader\n",
    "        :param num_epochs: The number of epochs to learn\n",
    "        :param lr: The learning rate\n",
    "        :param save_filename: The filename to save the model pt file\n",
    "        :param device: The device to use ('cpu' or 'cuda')\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, (x_batch, y_batch) in enumerate(tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device=device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                seq_len = x_batch.size(1)\n",
    "                batch_size = x_batch.size(0)\n",
    "                num_heads = self.model.blocks[0].attn.num_heads  # or set as self.n_heads earlier\n",
    "\n",
    "                mask = generate_causal_mask(seq_len, batch_size, num_heads, device)\n",
    "\n",
    "                logits = self.model(x_batch, mask=mask)\n",
    "                logits = logits.view(-1, logits.size(-1))\n",
    "                y_batch = y_batch.view(-1)\n",
    "\n",
    "                loss = criterion(logits, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            epoch_loss = running_loss / len(loader)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        torch.save(self.model.state_dict(), save_filename)\n",
    "\n",
    "    def test(self, loader, checkpoint='GPT2TW.pt', device='cuda'):\n",
    "        if checkpoint:\n",
    "            self.model.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0 # why\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (x_batch, y_batch) in enumerate(loader):\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device=device)\n",
    "\n",
    "                seq_len = x_batch.size(1)\n",
    "                batch_size = x_batch.size(0)\n",
    "                num_heads = self.model.blocks[0].attn.num_heads\n",
    "                mask = generate_causal_mask(seq_len, batch_size, num_heads, device)\n",
    "\n",
    "                logits = self.model(x_batch, mask=mask)\n",
    "                logits = logits.view(-1, logits.size(-1))\n",
    "                y_batch = y_batch.view(-1)\n",
    "\n",
    "                loss = criterion(logits, y_batch)\n",
    "                total_loss += loss.item()\n",
    "                total_tokens += y_batch.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Test Perplexity: {perplexity:.4f}\")\n",
    "        return avg_loss, perplexity\n",
    "\n",
    "    def generate_text(self, job_title, summarized_jd, max_len=100, device='cpu'):\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "\n",
    "        prompt_text = (\n",
    "            f\"Job Title: {job_title}\\n\"\n",
    "            f\"Preferred Qualifications: {summarized_jd}\\n\"\n",
    "            f\"Hiring Company: Apple\\n\"\n",
    "            f\"Applicant Name: Tracy Wu\\n\"\n",
    "            f\"Past Working Experience: 3 internships in Data Science\\n\"\n",
    "            f\"Current Working Experience: Data Scientist Intern\\n\"\n",
    "            f\"Skillsets: Python, SQL, R, AWS, Azure\\n\"\n",
    "            f\"Qualifications: Master of Science in Statistics from University of Michigan\\n\"\n",
    "            \"Cover Letter: \\n\"\n",
    "        )\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            prompt_text,\n",
    "            max_length=max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encodings.input_ids.squeeze(0)\n",
    "        input_tensors = input_ids.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_tensor = self.model.generate(\n",
    "                input_tensors,\n",
    "                max_len=max_len)\n",
    "\n",
    "        output_ids = output_tensor[0].tolist()\n",
    "        return self.tokenizer.decode(output_ids, skip_special_tokens=True)\n"
   ],
   "id": "68055f85efabec4e",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T02:37:40.070542Z",
     "start_time": "2025-04-21T02:37:38.794168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "vocab_size =  50257 # Vocabulary size\n",
    "batch_size = 2\n",
    "block_size = 200    # Sequence length (Context) for prediction, make sure it is equal or bigger than max_length in Dataset\n",
    "n_layers = 6      # Number of transformer layers\n",
    "n_heads = 6      # Number of attention heads\n",
    "n_embds = 384      # Embedding size\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 10   # Number of training epochs\n",
    "eval_iters = 200\n",
    "eval_interval = 500\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "df_cl_train = pd.read_csv(\"/Users/tracy/Desktop/留学/UMich/SI 630/Final Project/Data/CoverLetter_train.csv\")\n",
    "df_cl_test = pd.read_csv(\"/Users/tracy/Desktop/留学/UMich/SI 630/Final Project/Data/CoverLetter_test.csv\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "dataset_train = CoverLetterDataset(df_cl_train, tokenizer)\n",
    "dataset_test = CoverLetterDataset(df_cl_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=2, shuffle=True)\n",
    "\n",
    "trainer = GPTTrainer(vocab_size=vocab_size, block_size=block_size, n_layers=n_layers, n_heads=n_heads, n_embds=n_embds, tokenizer=tokenizer)\n"
   ],
   "id": "6cf2984640c9f682",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T02:56:04.706212Z",
     "start_time": "2025-04-21T02:37:40.083837Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(train_loader, num_epochs=num_epochs, lr=learning_rate, device='cpu')",
   "id": "6fd372c3ff0132d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 407/407 [01:52<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 6.2397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 407/407 [01:54<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 2.3243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 407/407 [01:50<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 1.3385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 407/407 [01:49<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.9373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 407/407 [01:50<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.7104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 407/407 [01:49<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.5601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 407/407 [01:49<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.4556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 407/407 [01:49<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.3785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 407/407 [01:50<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.3173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 407/407 [01:49<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.2697\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T02:56:07.870872Z",
     "start_time": "2025-04-21T02:56:04.737709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer.generate_text(\n",
    "    job_title=\"Data Analyst\",\n",
    "    summarized_jd=\"Automate Data pipelines and conduct data analysis\",\n",
    "    max_len=100,\n",
    "    device=device,\n",
    ")"
   ],
   "id": "4b6a309ef099593c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job Title: Data Analyst\\nPreferred Qualifications: Automate Data pipelines and conduct data analysis\\nHiring Company: Apple\\nApplicant Name: Tracy Wu\\nPast Working Experience: 3 internships in Data Science\\nCurrent Working Experience: Data Scientist Intern\\nSkillsets: Python, SQL, R, AWS, Azure\\nQualifications: Master of Science in Statistics from University of Michigan\\nPlease write a Cover Letter for Tracy Wu\\n understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding understanding gather performed analytical analytical analytical analytical analytical analytical analytical analyticalDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDearDear'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
